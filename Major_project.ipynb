{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13516796,
          "sourceType": "datasetVersion",
          "datasetId": 8582184
        },
        {
          "sourceId": 13557952,
          "sourceType": "datasetVersion",
          "datasetId": 8611718
        },
        {
          "sourceId": 13558376,
          "sourceType": "datasetVersion",
          "datasetId": 8612034
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Major project",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Domaakshithareddy/NeuroSpeech/blob/main/Major_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "CP2U2_d9noVd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "akshithawork_cloud_json_path = kagglehub.dataset_download('akshithawork/cloud-json')\n",
        "akshithawork_librispeech_100_path = kagglehub.dataset_download('akshithawork/librispeech-100')\n",
        "akshithawork_wav2vec2_large_robust_local_path = kagglehub.dataset_download('akshithawork/wav2vec2-large-robust-local')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DjRH9801noVh"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# cell: detect_librispeech_and_preview.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# candidate roots to search for LibriSpeech\n",
        "candidates = [\n",
        "    Path(\"/kaggle/input\"),\n",
        "    Path(\"/kaggle/working\"),\n",
        "    Path(\"/kaggle/working/data\"),\n",
        "    Path(\"/mnt/data\"),\n",
        "    Path(\".\")\n",
        "]\n",
        "\n",
        "found = None\n",
        "for cand in candidates:\n",
        "    if cand.exists():\n",
        "        # search for a folder named 'LibriSpeech' or 'train-clean-100' directly\n",
        "        libdir = cand / \"LibriSpeech\"\n",
        "        if libdir.exists() and (libdir / \"train-clean-100\").exists():\n",
        "            found = libdir\n",
        "            break\n",
        "        # maybe they uploaded as top-level train-clean-100\n",
        "        if (cand / \"train-clean-100\").exists():\n",
        "            found = cand\n",
        "            break\n",
        "\n",
        "if not found:\n",
        "    # fallback: search recursively for a train-clean-100 folder\n",
        "    for root, dirs, files in os.walk(\"/kaggle\", topdown=True):\n",
        "        if \"train-clean-100\" in dirs:\n",
        "            found = Path(root)\n",
        "            break\n",
        "    if not found:\n",
        "        for root, dirs, files in os.walk(\"/mnt\", topdown=True):\n",
        "            if \"train-clean-100\" in dirs:\n",
        "                found = Path(root)\n",
        "                break\n",
        "\n",
        "if not found:\n",
        "    print(\"Could not auto-detect LibriSpeech. Please paste the full path to the folder containing 'train-clean-100'.\")\n",
        "else:\n",
        "    print(\"Detected dataset root:\", found)\n",
        "    t = (found / \"train-clean-100\")\n",
        "    print(\"train-clean-100 exists:\", t.exists())\n",
        "    # list first-level children and show sample audio files for one speaker\n",
        "    print(\"\\nTop-level entries in dataset root:\")\n",
        "    for p in sorted(found.iterdir()):\n",
        "        print(\" -\", p.name, \"(dir)\" if p.is_dir() else \"(file)\", )\n",
        "    # show a few speaker folders\n",
        "    if t.exists():\n",
        "        speakers = sorted([d for d in t.iterdir() if d.is_dir()])[:8]\n",
        "        print(\"\\nSample speaker folders (first 8):\", [s.name for s in speakers])\n",
        "        # show one speaker/chapter contents\n",
        "        if speakers:\n",
        "            sp = speakers[0]\n",
        "            chapters = sorted([d for d in sp.iterdir() if d.is_dir()])[:4]\n",
        "            print(f\"\\nFor speaker {sp.name}, show up to 4 chapters:\", [c.name for c in chapters])\n",
        "            if chapters:\n",
        "                ch = chapters[0]\n",
        "                files = sorted(list(ch.glob(\"*.*\")))[:10]\n",
        "                print(f\"\\nExample files in {sp.name}/{ch.name}:\")\n",
        "                for f in files:\n",
        "                    print(\"   \", f.name, \"-\", f.stat().st_size, \"bytes\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T15:18:50.246397Z",
          "iopub.execute_input": "2025-10-30T15:18:50.24669Z",
          "iopub.status.idle": "2025-10-30T15:18:50.703237Z",
          "shell.execute_reply.started": "2025-10-30T15:18:50.246661Z",
          "shell.execute_reply": "2025-10-30T15:18:50.701982Z"
        },
        "id": "LJgFtHjznoVi",
        "outputId": "c41edd8d-767a-4b29-a585-ebe2db9b3795"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Detected dataset root: /kaggle/input/librispeech-100/LibriSpeech\ntrain-clean-100 exists: True\n\nTop-level entries in dataset root:\n - BOOKS.TXT (file)\n - CHAPTERS.TXT (file)\n - LICENSE.TXT (file)\n - README.TXT (file)\n - SPEAKERS.TXT (file)\n - train-clean-100 (dir)\n\nSample speaker folders (first 8): ['103', '1034', '1040', '1069', '1081', '1088', '1098', '1116']\n\nFor speaker 103, show up to 4 chapters: ['1240', '1241']\n\nExample files in 103/1240:\n    103-1240-0000.flac - 255398 bytes\n    103-1240-0001.flac - 301627 bytes\n    103-1240-0002.flac - 270539 bytes\n    103-1240-0003.flac - 277620 bytes\n    103-1240-0004.flac - 239259 bytes\n    103-1240-0005.flac - 295296 bytes\n    103-1240-0006.flac - 183331 bytes\n    103-1240-0007.flac - 281327 bytes\n    103-1240-0008.flac - 287677 bytes\n    103-1240-0009.flac - 196792 bytes\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Build manifest directly from .trans.txt files (robust, no torchaudio)\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "dataset_root = Path(\"/kaggle/input/librispeech-100/LibriSpeech\")\n",
        "split = \"train-clean-100\"\n",
        "split_dir = dataset_root / split\n",
        "if not split_dir.exists():\n",
        "    raise RuntimeError(f\"Expected {split_dir} to exist. Adjust dataset_root if needed.\")\n",
        "\n",
        "# 1) collect all .trans.txt files and build mapping utterance_id -> transcript\n",
        "trans_map = {}\n",
        "trans_files = list(split_dir.glob(\"**/*.trans.txt\"))\n",
        "print(f\"Found {len(trans_files)} .trans.txt files (chapter-level transcript files).\")\n",
        "\n",
        "for tf in trans_files:\n",
        "    try:\n",
        "        with tf.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                # format: \"<utterance-id> <transcript...>\"\n",
        "                parts = line.split(\" \", 1)\n",
        "                if len(parts) == 2:\n",
        "                    uttid, text = parts\n",
        "                    trans_map[uttid] = text\n",
        "    except Exception as e:\n",
        "        print(\"Warning: failed reading\", tf, \":\", e)\n",
        "\n",
        "print(\"Total utterances in transcripts mapping:\", len(trans_map))\n",
        "\n",
        "# 2) find all .flac files and match to trans_map\n",
        "flac_files = list(split_dir.glob(\"**/*.flac\"))\n",
        "print(\"Found\", len(flac_files), \"flac files under\", split_dir)\n",
        "\n",
        "manifest_out = Path(\"manifests\")\n",
        "manifest_out.mkdir(exist_ok=True)\n",
        "out_file = manifest_out / f\"{split}.tsv\"\n",
        "\n",
        "write_count = 0\n",
        "missing_count = 0\n",
        "with out_file.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"audio_path\", \"transcript\"])\n",
        "    for fl in sorted(flac_files):\n",
        "        # utterance id is filename without extension (e.g., 103-1240-0000)\n",
        "        uttid = fl.stem\n",
        "        if uttid in trans_map:\n",
        "            writer.writerow([str(fl), trans_map[uttid]])\n",
        "            write_count += 1\n",
        "        else:\n",
        "            missing_count += 1\n",
        "            # Optionally write with empty transcript or skip. We'll skip but log.\n",
        "            # writer.writerow([str(fl), \"\"])\n",
        "if missing_count:\n",
        "    print(f\"Warning: {missing_count} flac files had no matching transcript (they were skipped).\")\n",
        "print(f\"Wrote {write_count} entries to {out_file.resolve()}\")\n",
        "\n",
        "# 3) print a few sample rows\n",
        "print(\"\\nSample manifest lines:\")\n",
        "with out_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "        if i >= 5: break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T15:22:05.845709Z",
          "iopub.execute_input": "2025-10-30T15:22:05.846369Z",
          "iopub.status.idle": "2025-10-30T15:22:53.450568Z",
          "shell.execute_reply.started": "2025-10-30T15:22:05.846339Z",
          "shell.execute_reply": "2025-10-30T15:22:53.449758Z"
        },
        "id": "jZTlTOvQnoVj",
        "outputId": "6cccf5b8-d71e-4226-a392-eb82b133980e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 585 .trans.txt files (chapter-level transcript files).\nTotal utterances in transcripts mapping: 28539\nFound 28539 flac files under /kaggle/input/librispeech-100/LibriSpeech/train-clean-100\nWrote 28539 entries to /kaggle/working/manifests/train-clean-100.tsv\n\nSample manifest lines:\naudio_path\ttranscript\n/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0000.flac\tCHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK\n/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0001.flac\tTHAT HAD ITS SOURCE AWAY BACK IN THE WOODS OF THE OLD CUTHBERT PLACE IT WAS REPUTED TO BE AN INTRICATE HEADLONG BROOK IN ITS EARLIER COURSE THROUGH THOSE WOODS WITH DARK SECRETS OF POOL AND CASCADE BUT BY THE TIME IT REACHED LYNDE'S HOLLOW IT WAS A QUIET WELL CONDUCTED LITTLE STREAM\n/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0002.flac\tFOR NOT EVEN A BROOK COULD RUN PAST MISSUS RACHEL LYNDE'S DOOR WITHOUT DUE REGARD FOR DECENCY AND DECORUM IT PROBABLY WAS CONSCIOUS THAT MISSUS RACHEL WAS SITTING AT HER WINDOW KEEPING A SHARP EYE ON EVERYTHING THAT PASSED FROM BROOKS AND CHILDREN UP\n/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0003.flac\tAND THAT IF SHE NOTICED ANYTHING ODD OR OUT OF PLACE SHE WOULD NEVER REST UNTIL SHE HAD FERRETED OUT THE WHYS AND WHEREFORES THEREOF THERE ARE PLENTY OF PEOPLE IN AVONLEA AND OUT OF IT WHO CAN ATTEND CLOSELY TO THEIR NEIGHBOR'S BUSINESS BY DINT OF NEGLECTING THEIR OWN\n/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0004.flac\tBUT MISSUS RACHEL LYNDE WAS ONE OF THOSE CAPABLE CREATURES WHO CAN MANAGE THEIR OWN CONCERNS AND THOSE OF OTHER FOLKS INTO THE BARGAIN SHE WAS A NOTABLE HOUSEWIFE HER WORK WAS ALWAYS DONE AND WELL DONE SHE RAN THE SEWING CIRCLE\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: env & imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2Processor\n",
        "import nltk\n",
        "nltk.download(\"cmudict\", quiet=True)\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Paths (edit if your locations differ)\n",
        "DATA_ROOT = Path(\"/kaggle/input/librispeech-100/LibriSpeech\")   # where the audio folders are\n",
        "MANIFEST_PATH = Path(\"/kaggle/working/manifests/train-clean-100.tsv\")          # your uploaded manifest in this chat\n",
        "\n",
        "print(\"DATA_ROOT exists:\", DATA_ROOT.exists(), \"  path:\", DATA_ROOT)\n",
        "print(\"MANIFEST_PATH exists:\", MANIFEST_PATH.exists(), \"  path:\", MANIFEST_PATH)\n",
        "\n",
        "# quick device print\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T15:27:35.628891Z",
          "iopub.execute_input": "2025-10-30T15:27:35.629193Z",
          "iopub.status.idle": "2025-10-30T15:27:36.800095Z",
          "shell.execute_reply.started": "2025-10-30T15:27:35.629174Z",
          "shell.execute_reply": "2025-10-30T15:27:36.799301Z"
        },
        "id": "VS6ePQnonoVk",
        "outputId": "c096f04e-adfe-4e34-fd99-ce62614d716a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DATA_ROOT exists: True   path: /kaggle/input/librispeech-100/LibriSpeech\nMANIFEST_PATH exists: True   path: /kaggle/working/manifests/train-clean-100.tsv\nDevice: cpu\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "\n",
        "LOCAL = \"/kaggle/input/wav2vec2-large-robust-local/wav2vec2-large-robust-local\"  # or wherever you unzipped it\n",
        "model = Wav2Vec2Model.from_pretrained(LOCAL, local_files_only=True)\n",
        "fe = Wav2Vec2FeatureExtractor.from_pretrained(LOCAL, local_files_only=True)\n",
        "\n",
        "print(\"Model loaded:\", model.config.hidden_size)\n",
        "print(\"Feature extractor sr:\", fe.sampling_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T15:58:30.410767Z",
          "iopub.execute_input": "2025-10-30T15:58:30.411147Z",
          "iopub.status.idle": "2025-10-30T15:58:42.752109Z",
          "shell.execute_reply.started": "2025-10-30T15:58:30.411121Z",
          "shell.execute_reply": "2025-10-30T15:58:42.751079Z"
        },
        "id": "nzPVPduqnoVk",
        "outputId": "365b93dd-b6a1-4f14-94e3-91b99a1b3ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model loaded: 1024\nFeature extractor sr: 16000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Single cell: small end-to-end sanity check (one prompt)\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"cmudict\", quiet=True)\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# --- CONFIG: adjust if needed ---\n",
        "# manifest may be in /kaggle/working/manifests or /mnt/data if you uploaded it here\n",
        "manifest_candidates = [\n",
        "    Path(\"/kaggle/working/manifests/train-clean-100.tsv\"),\n",
        "    Path(\"/mnt/data/train-clean-100.tsv\"),\n",
        "    Path(\"manifests/train-clean-100.tsv\")\n",
        "]\n",
        "MANIFEST = next((p for p in manifest_candidates if p.exists()), None)\n",
        "if MANIFEST is None:\n",
        "    raise FileNotFoundError(\"Manifest not found. Expected at one of: \" + \", \".join(str(p) for p in manifest_candidates))\n",
        "print(\"Using manifest:\", MANIFEST)\n",
        "\n",
        "# small utilities: CMUdict phonemizer + minimal phoneme->features (covers common ARPAbet)\n",
        "cmu = cmudict.dict()\n",
        "def word_to_phonemes(word):\n",
        "    w = word.lower()\n",
        "    if w in cmu:\n",
        "        phones = cmu[w][0]\n",
        "        return [p.rstrip(\"012\") for p in phones]\n",
        "    return []\n",
        "\n",
        "def sentence_to_phonemes(sentence):\n",
        "    words = [w.strip(\".,!?;:()[]'\\\"\").lower() for w in sentence.split()]\n",
        "    phs = []\n",
        "    for w in words:\n",
        "        phs += word_to_phonemes(w)\n",
        "    return phs\n",
        "\n",
        "FEATURE_LIST = [\n",
        " 'consonant','sonorant','fricative','nasal','stop','approximant','affricate','liquid','vowel','semivowel','continuant',\n",
        " 'alveolar','palatal','dental','glottal','labial','velar','mid','high','low','front','back','central','anterior','posterior','retroflex','bilabial','coronal','dorsal',\n",
        " 'long','short','monophthong','diphthong','round','voiced'\n",
        "]\n",
        "N_FEATURES = len(FEATURE_LIST)\n",
        "from collections import defaultdict\n",
        "PHONEME_TO_FEATURES = defaultdict(set)\n",
        "def add(ph, *feats):\n",
        "    PHONEME_TO_FEATURES[ph].update(feats)\n",
        "\n",
        "# minimal mapping (covers standard ARPAbet used by CMUdict)\n",
        "# vowels\n",
        "add('AA','vowel','low','back','monophthong'); add('AE','vowel','low','front','monophthong')\n",
        "add('AH','vowel','mid','central','short','monophthong'); add('AO','vowel','low','back','monophthong')\n",
        "add('AW','vowel','diphthong','back'); add('AY','vowel','diphthong','front')\n",
        "add('EH','vowel','mid','front','monophthong'); add('ER','vowel','mid','central','monophthong','round')\n",
        "add('EY','vowel','diphthong','front'); add('IH','vowel','high','front','short')\n",
        "add('IY','vowel','high','front','long','monophthong'); add('OW','vowel','diphthong','back')\n",
        "add('OY','vowel','diphthong','back'); add('UH','vowel','high','back','short'); add('UW','vowel','high','back','long','round')\n",
        "# stops\n",
        "add('P','consonant','stop','bilabial'); add('B','consonant','stop','bilabial','voiced')\n",
        "add('T','consonant','stop','alveolar'); add('D','consonant','stop','alveolar','voiced')\n",
        "add('K','consonant','stop','velar'); add('G','consonant','stop','velar','voiced')\n",
        "# fricatives\n",
        "add('F','consonant','fricative','labial'); add('V','consonant','fricative','labial','voiced')\n",
        "add('TH','consonant','fricative','dental'); add('DH','consonant','fricative','dental','voiced')\n",
        "add('S','consonant','fricative','alveolar'); add('Z','consonant','fricative','alveolar','voiced')\n",
        "add('SH','consonant','fricative','palatal'); add('ZH','consonant','fricative','palatal','voiced'); add('HH','consonant','fricative','glottal')\n",
        "# affricates / nasals / approximants\n",
        "add('CH','consonant','affricate','palatal'); add('JH','consonant','affricate','palatal','voiced')\n",
        "add('M','consonant','nasal','bilabial','voiced'); add('N','consonant','nasal','alveolar','voiced'); add('NG','consonant','nasal','velar','voiced')\n",
        "add('L','consonant','liquid','alveolar','voiced'); add('R','consonant','liquid','coronal','voiced')\n",
        "add('W','consonant','semivowel','labial','voiced'); add('Y','consonant','semivowel','palatal','voiced')\n",
        "\n",
        "def phonemes_to_feature_sequences(phonemes):\n",
        "    \"\"\"Return list of N_FEATURES lists of global token ids: +att=2*i, -att=2*i+1\"\"\"\n",
        "    seqs = [[] for _ in range(N_FEATURES)]\n",
        "    for ph in phonemes:\n",
        "        feats = PHONEME_TO_FEATURES.get(ph, set())\n",
        "        for i, feat in enumerate(FEATURE_LIST):\n",
        "            if feat in feats:\n",
        "                seqs[i].append(2*i)\n",
        "            else:\n",
        "                seqs[i].append(2*i+1)\n",
        "    return seqs\n",
        "\n",
        "# --- Dataset (reads manifest, returns waveform numpy and phonemes) ---\n",
        "class SmallLibriDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, manifest_tsv, max_items=None):\n",
        "        self.rows = []\n",
        "        with open(manifest_tsv, \"r\", encoding=\"utf-8\") as f:\n",
        "            rdr = csv.DictReader(f, delimiter=\"\\t\")\n",
        "            for i,row in enumerate(rdr):\n",
        "                self.rows.append((row['audio_path'], row['transcript']))\n",
        "                if max_items and i+1>=max_items:\n",
        "                    break\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "    def __getitem__(self, idx):\n",
        "        path, transcript = self.rows[idx]\n",
        "        waveform, sr = torchaudio.load(path)\n",
        "        if sr != fe.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, fe.sampling_rate)\n",
        "        if waveform.dim()>1:\n",
        "            waveform = waveform.mean(dim=0)\n",
        "        # return numpy 1D float32 (feature extractor accepts list of arrays)\n",
        "        wav_np = waveform.numpy().astype(np.float32)\n",
        "        # phonemes\n",
        "        phs = sentence_to_phonemes(transcript)\n",
        "        feat_seqs = phonemes_to_feature_sequences(phs)\n",
        "        return wav_np, phs, feat_seqs, path\n",
        "\n",
        "# collate: use feature-extractor `fe` to get input_values & attention_mask (pt tensors)\n",
        "def collate_batch(batch):\n",
        "    wavs = [item[0] for item in batch]  # numpy arrays\n",
        "    phs = [item[1] for item in batch]\n",
        "    feat_seqs = [item[2] for item in batch]\n",
        "    paths = [item[3] for item in batch]\n",
        "    inputs = fe(wavs, sampling_rate=fe.sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "    return inputs, feat_seqs, phs, paths\n",
        "\n",
        "# --- create small dataloader (2 examples) ---\n",
        "ds = SmallLibriDataset(MANIFEST, max_items=200)   # limit to 200 reading time\n",
        "print(\"Dataset size (using max_items=200):\", len(ds))\n",
        "dl = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# --- build linear head and device setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "# freeze feature extractor of wav2vec if not already\n",
        "for p in model.feature_extractor.parameters():\n",
        "    p.requires_grad = False\n",
        "hidden = model.config.hidden_size\n",
        "out_dim = 2 * N_FEATURES + 1\n",
        "linear_head = nn.Linear(hidden, out_dim).to(device)\n",
        "model.to(device).eval()\n",
        "\n",
        "# take one batch and run forward\n",
        "batch_inputs, batch_feat_seqs, batch_phs, batch_paths = next(iter(dl))\n",
        "input_values = batch_inputs[\"input_values\"].to(device)   # shape (B, L)\n",
        "attention_mask = batch_inputs.get(\"attention_mask\", None)\n",
        "if attention_mask is not None:\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    wav2_out = model(input_values, attention_mask=attention_mask)\n",
        "    hs = wav2_out.last_hidden_state     # (B, T, H)\n",
        "    logits = linear_head(hs)            # (B, T, C)\n",
        "\n",
        "print(\"Batch audio paths:\", batch_paths)\n",
        "print(\"Example phonemes (sample 0):\", batch_phs[0][:60])\n",
        "print(\"Example feature-seq lengths for sample 0 (per-feature):\", [len(s) for s in batch_feat_seqs[0][:8]], \" ...\")\n",
        "print(\"Hidden states shape (B, T, H):\", hs.shape)\n",
        "print(\"Logits shape (B, T, C):\", logits.shape, \" expected C:\", out_dim)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-30T16:45:31.619084Z",
          "iopub.execute_input": "2025-10-30T16:45:31.62238Z",
          "iopub.status.idle": "2025-10-30T16:45:49.643711Z",
          "shell.execute_reply.started": "2025-10-30T16:45:31.62229Z",
          "shell.execute_reply": "2025-10-30T16:45:49.642392Z"
        },
        "id": "IHAqH6-EnoVk",
        "outputId": "44af9ccd-7152-47ac-ed0b-8d9afb64bb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using manifest: /kaggle/working/manifests/train-clean-100.tsv\nDataset size (using max_items=200): 200\nDevice: cpu\nBatch audio paths: ['/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0020.flac', '/kaggle/input/librispeech-100/LibriSpeech/train-clean-100/103/1240/103-1240-0054.flac']\nExample phonemes (sample 0): ['B', 'AO', 'R', 'D', 'ER', 'D', 'W', 'IH', 'DH', 'W', 'AY', 'L', 'D', 'R', 'OW', 'Z', 'B', 'UH', 'SH', 'AH', 'Z', 'IH', 'T', 'S', 'N', 'OW', 'W', 'AH', 'N', 'D', 'ER', 'M', 'AE', 'TH', 'Y', 'UW', 'AH', 'N', 'D', 'M', 'AA', 'R', 'IH', 'L', 'AH', 'AA', 'R', 'B', 'OW', 'TH', 'AH', 'L', 'IH', 'T', 'AH', 'L', 'AA', 'D', 'L', 'IH']\nExample feature-seq lengths for sample 0 (per-feature): [153, 153, 153, 153, 153, 153, 153, 153]  ...\nHidden states shape (B, T, H): torch.Size([2, 816, 1024])\nLogits shape (B, T, C): torch.Size([2, 816, 71])  expected C: 71\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "21mvN08_noVl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
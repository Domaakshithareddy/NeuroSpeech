{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13516796,
          "sourceType": "datasetVersion",
          "datasetId": 8582184
        },
        {
          "sourceId": 13557952,
          "sourceType": "datasetVersion",
          "datasetId": 8611718
        },
        {
          "sourceId": 13558376,
          "sourceType": "datasetVersion",
          "datasetId": 8612034
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Major project",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Domaakshithareddy/NeuroSpeech/blob/main/Major_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell: detect_librispeech_and_preview.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# candidate roots to search for LibriSpeech\n",
        "candidates = [\n",
        "    Path(\"/kaggle/input\"),\n",
        "    Path(\"/kaggle/working\"),\n",
        "    Path(\"/kaggle/working/data\"),\n",
        "    Path(\"/mnt/data\"),\n",
        "    Path(\".\")\n",
        "]\n",
        "\n",
        "found = None\n",
        "for cand in candidates:\n",
        "    if cand.exists():\n",
        "        # search for a folder named 'LibriSpeech' or 'train-clean-100' directly\n",
        "        libdir = cand / \"LibriSpeech\"\n",
        "        if libdir.exists() and (libdir / \"train-clean-100\").exists():\n",
        "            found = libdir\n",
        "            break\n",
        "        # maybe they uploaded as top-level train-clean-100\n",
        "        if (cand / \"train-clean-100\").exists():\n",
        "            found = cand\n",
        "            break\n",
        "\n",
        "if not found:\n",
        "    # fallback: search recursively for a train-clean-100 folder\n",
        "    for root, dirs, files in os.walk(\"/kaggle\", topdown=True):\n",
        "        if \"train-clean-100\" in dirs:\n",
        "            found = Path(root)\n",
        "            break\n",
        "    if not found:\n",
        "        for root, dirs, files in os.walk(\"/mnt\", topdown=True):\n",
        "            if \"train-clean-100\" in dirs:\n",
        "                found = Path(root)\n",
        "                break\n",
        "\n",
        "if not found:\n",
        "    print(\"Could not auto-detect LibriSpeech. Please paste the full path to the folder containing 'train-clean-100'.\")\n",
        "else:\n",
        "    print(\"Detected dataset root:\", found)\n",
        "    t = (found / \"train-clean-100\")\n",
        "    print(\"train-clean-100 exists:\", t.exists())\n",
        "    # list first-level children and show sample audio files for one speaker\n",
        "    print(\"\\nTop-level entries in dataset root:\")\n",
        "    for p in sorted(found.iterdir()):\n",
        "        print(\" -\", p.name, \"(dir)\" if p.is_dir() else \"(file)\", )\n",
        "    # show a few speaker folders\n",
        "    if t.exists():\n",
        "        speakers = sorted([d for d in t.iterdir() if d.is_dir()])[:8]\n",
        "        print(\"\\nSample speaker folders (first 8):\", [s.name for s in speakers])\n",
        "        # show one speaker/chapter contents\n",
        "        if speakers:\n",
        "            sp = speakers[0]\n",
        "            chapters = sorted([d for d in sp.iterdir() if d.is_dir()])[:4]\n",
        "            print(f\"\\nFor speaker {sp.name}, show up to 4 chapters:\", [c.name for c in chapters])\n",
        "            if chapters:\n",
        "                ch = chapters[0]\n",
        "                files = sorted(list(ch.glob(\"*.*\")))[:10]\n",
        "                print(f\"\\nExample files in {sp.name}/{ch.name}:\")\n",
        "                for f in files:\n",
        "                    print(\"   \", f.name, \"-\", f.stat().st_size, \"bytes\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:18:29.150557Z",
          "iopub.execute_input": "2025-11-07T12:18:29.150802Z",
          "iopub.status.idle": "2025-11-07T12:18:29.603254Z",
          "shell.execute_reply.started": "2025-11-07T12:18:29.150783Z",
          "shell.execute_reply": "2025-11-07T12:18:29.60262Z"
        },
        "id": "2FR3pST198Hf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Build manifest directly from .trans.txt files (robust, no torchaudio)\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "dataset_root = Path(\"/kaggle/input/librispeech-100/LibriSpeech\")\n",
        "split = \"train-clean-100\"\n",
        "split_dir = dataset_root / split\n",
        "if not split_dir.exists():\n",
        "    raise RuntimeError(f\"Expected {split_dir} to exist. Adjust dataset_root if needed.\")\n",
        "\n",
        "# 1) collect all .trans.txt files and build mapping utterance_id -> transcript\n",
        "trans_map = {}\n",
        "trans_files = list(split_dir.glob(\"**/*.trans.txt\"))\n",
        "print(f\"Found {len(trans_files)} .trans.txt files (chapter-level transcript files).\")\n",
        "\n",
        "for tf in trans_files:\n",
        "    try:\n",
        "        with tf.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                # format: \"<utterance-id> <transcript...>\"\n",
        "                parts = line.split(\" \", 1)\n",
        "                if len(parts) == 2:\n",
        "                    uttid, text = parts\n",
        "                    trans_map[uttid] = text\n",
        "    except Exception as e:\n",
        "        print(\"Warning: failed reading\", tf, \":\", e)\n",
        "\n",
        "print(\"Total utterances in transcripts mapping:\", len(trans_map))\n",
        "\n",
        "# 2) find all .flac files and match to trans_map\n",
        "flac_files = list(split_dir.glob(\"**/*.flac\"))\n",
        "print(\"Found\", len(flac_files), \"flac files under\", split_dir)\n",
        "\n",
        "manifest_out = Path(\"manifests\")\n",
        "manifest_out.mkdir(exist_ok=True)\n",
        "out_file = manifest_out / f\"{split}.tsv\"\n",
        "\n",
        "write_count = 0\n",
        "missing_count = 0\n",
        "with out_file.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f, delimiter=\"\\t\")\n",
        "    writer.writerow([\"audio_path\", \"transcript\"])\n",
        "    for fl in sorted(flac_files):\n",
        "        # utterance id is filename without extension (e.g., 103-1240-0000)\n",
        "        uttid = fl.stem\n",
        "        if uttid in trans_map:\n",
        "            writer.writerow([str(fl), trans_map[uttid]])\n",
        "            write_count += 1\n",
        "        else:\n",
        "            missing_count += 1\n",
        "            # Optionally write with empty transcript or skip. We'll skip but log.\n",
        "            # writer.writerow([str(fl), \"\"])\n",
        "if missing_count:\n",
        "    print(f\"Warning: {missing_count} flac files had no matching transcript (they were skipped).\")\n",
        "print(f\"Wrote {write_count} entries to {out_file.resolve()}\")\n",
        "\n",
        "# 3) print a few sample rows\n",
        "print(\"\\nSample manifest lines:\")\n",
        "with out_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(line.strip())\n",
        "        if i >= 5: break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:18:29.604147Z",
          "iopub.execute_input": "2025-11-07T12:18:29.604396Z",
          "iopub.status.idle": "2025-11-07T12:19:12.604815Z",
          "shell.execute_reply.started": "2025-11-07T12:18:29.604372Z",
          "shell.execute_reply": "2025-11-07T12:19:12.604181Z"
        },
        "id": "s03HHPQj98Ho"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: env & imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2Processor\n",
        "import nltk\n",
        "nltk.download(\"cmudict\", quiet=True)\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Paths (edit if your locations differ)\n",
        "DATA_ROOT = Path(\"/kaggle/input/librispeech-100/LibriSpeech\")   # where the audio folders are\n",
        "MANIFEST_PATH = Path(\"/kaggle/working/manifests/train-clean-100.tsv\")          # your uploaded manifest in this chat\n",
        "\n",
        "print(\"DATA_ROOT exists:\", DATA_ROOT.exists(), \"  path:\", DATA_ROOT)\n",
        "print(\"MANIFEST_PATH exists:\", MANIFEST_PATH.exists(), \"  path:\", MANIFEST_PATH)\n",
        "\n",
        "# quick device print\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:19:12.605414Z",
          "iopub.execute_input": "2025-11-07T12:19:12.605576Z",
          "iopub.status.idle": "2025-11-07T12:19:37.178092Z",
          "shell.execute_reply.started": "2025-11-07T12:19:12.605562Z",
          "shell.execute_reply": "2025-11-07T12:19:37.177257Z"
        },
        "id": "LceNu8Pc98Hp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "\n",
        "LOCAL = \"/kaggle/input/wav2vec2-large-robust-local/wav2vec2-large-robust-local\"  # or wherever you unzipped it\n",
        "model = Wav2Vec2Model.from_pretrained(LOCAL, local_files_only=True)\n",
        "fe = Wav2Vec2FeatureExtractor.from_pretrained(LOCAL, local_files_only=True)\n",
        "\n",
        "print(\"Model loaded:\", model.config.hidden_size)\n",
        "print(\"Feature extractor sr:\", fe.sampling_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:19:37.17972Z",
          "iopub.execute_input": "2025-11-07T12:19:37.180538Z",
          "iopub.status.idle": "2025-11-07T12:19:44.311464Z",
          "shell.execute_reply.started": "2025-11-07T12:19:37.180518Z",
          "shell.execute_reply": "2025-11-07T12:19:44.310851Z"
        },
        "id": "fe2A2yh_98Hq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Single cell: small end-to-end sanity check (one prompt)\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download(\"cmudict\", quiet=True)\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# --- CONFIG: adjust if needed ---\n",
        "# manifest may be in /kaggle/working/manifests or /mnt/data if you uploaded it here\n",
        "manifest_candidates = [\n",
        "    Path(\"/kaggle/working/manifests/train-clean-100.tsv\"),\n",
        "    Path(\"/mnt/data/train-clean-100.tsv\"),\n",
        "    Path(\"manifests/train-clean-100.tsv\")\n",
        "]\n",
        "MANIFEST = next((p for p in manifest_candidates if p.exists()), None)\n",
        "if MANIFEST is None:\n",
        "    raise FileNotFoundError(\"Manifest not found. Expected at one of: \" + \", \".join(str(p) for p in manifest_candidates))\n",
        "print(\"Using manifest:\", MANIFEST)\n",
        "\n",
        "# small utilities: CMUdict phonemizer + minimal phoneme->features (covers common ARPAbet)\n",
        "cmu = cmudict.dict()\n",
        "def word_to_phonemes(word):\n",
        "    w = word.lower()\n",
        "    if w in cmu:\n",
        "        phones = cmu[w][0]\n",
        "        return [p.rstrip(\"012\") for p in phones]\n",
        "    return []\n",
        "\n",
        "def sentence_to_phonemes(sentence):\n",
        "    words = [w.strip(\".,!?;:()[]'\\\"\").lower() for w in sentence.split()]\n",
        "    phs = []\n",
        "    for w in words:\n",
        "        phs += word_to_phonemes(w)\n",
        "    return phs\n",
        "\n",
        "FEATURE_LIST = [\n",
        " 'consonant','sonorant','fricative','nasal','stop','approximant','affricate','liquid','vowel','semivowel','continuant',\n",
        " 'alveolar','palatal','dental','glottal','labial','velar','mid','high','low','front','back','central','anterior','posterior','retroflex','bilabial','coronal','dorsal',\n",
        " 'long','short','monophthong','diphthong','round','voiced'\n",
        "]\n",
        "N_FEATURES = len(FEATURE_LIST)\n",
        "from collections import defaultdict\n",
        "PHONEME_TO_FEATURES = defaultdict(set)\n",
        "def add(ph, *feats):\n",
        "    PHONEME_TO_FEATURES[ph].update(feats)\n",
        "\n",
        "# minimal mapping (covers standard ARPAbet used by CMUdict)\n",
        "# vowels\n",
        "add('AA','vowel','low','back','monophthong'); add('AE','vowel','low','front','monophthong')\n",
        "add('AH','vowel','mid','central','short','monophthong'); add('AO','vowel','low','back','monophthong')\n",
        "add('AW','vowel','diphthong','back'); add('AY','vowel','diphthong','front')\n",
        "add('EH','vowel','mid','front','monophthong'); add('ER','vowel','mid','central','monophthong','round')\n",
        "add('EY','vowel','diphthong','front'); add('IH','vowel','high','front','short')\n",
        "add('IY','vowel','high','front','long','monophthong'); add('OW','vowel','diphthong','back')\n",
        "add('OY','vowel','diphthong','back'); add('UH','vowel','high','back','short'); add('UW','vowel','high','back','long','round')\n",
        "# stops\n",
        "add('P','consonant','stop','bilabial'); add('B','consonant','stop','bilabial','voiced')\n",
        "add('T','consonant','stop','alveolar'); add('D','consonant','stop','alveolar','voiced')\n",
        "add('K','consonant','stop','velar'); add('G','consonant','stop','velar','voiced')\n",
        "# fricatives\n",
        "add('F','consonant','fricative','labial'); add('V','consonant','fricative','labial','voiced')\n",
        "add('TH','consonant','fricative','dental'); add('DH','consonant','fricative','dental','voiced')\n",
        "add('S','consonant','fricative','alveolar'); add('Z','consonant','fricative','alveolar','voiced')\n",
        "add('SH','consonant','fricative','palatal'); add('ZH','consonant','fricative','palatal','voiced'); add('HH','consonant','fricative','glottal')\n",
        "# affricates / nasals / approximants\n",
        "add('CH','consonant','affricate','palatal'); add('JH','consonant','affricate','palatal','voiced')\n",
        "add('M','consonant','nasal','bilabial','voiced'); add('N','consonant','nasal','alveolar','voiced'); add('NG','consonant','nasal','velar','voiced')\n",
        "add('L','consonant','liquid','alveolar','voiced'); add('R','consonant','liquid','coronal','voiced')\n",
        "add('W','consonant','semivowel','labial','voiced'); add('Y','consonant','semivowel','palatal','voiced')\n",
        "\n",
        "def phonemes_to_feature_sequences(phonemes):\n",
        "    \"\"\"Return list of N_FEATURES lists of global token ids: +att=2*i, -att=2*i+1\"\"\"\n",
        "    seqs = [[] for _ in range(N_FEATURES)]\n",
        "    for ph in phonemes:\n",
        "        feats = PHONEME_TO_FEATURES.get(ph, set())\n",
        "        for i, feat in enumerate(FEATURE_LIST):\n",
        "            if feat in feats:\n",
        "                seqs[i].append(2*i)\n",
        "            else:\n",
        "                seqs[i].append(2*i+1)\n",
        "    return seqs\n",
        "\n",
        "# --- Dataset (reads manifest, returns waveform numpy and phonemes) ---\n",
        "class SmallLibriDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, manifest_tsv, max_items=None):\n",
        "        self.rows = []\n",
        "        with open(manifest_tsv, \"r\", encoding=\"utf-8\") as f:\n",
        "            rdr = csv.DictReader(f, delimiter=\"\\t\")\n",
        "            for i,row in enumerate(rdr):\n",
        "                self.rows.append((row['audio_path'], row['transcript']))\n",
        "                if max_items and i+1>=max_items:\n",
        "                    break\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "    def __getitem__(self, idx):\n",
        "        path, transcript = self.rows[idx]\n",
        "        waveform, sr = torchaudio.load(path)\n",
        "        if sr != fe.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, fe.sampling_rate)\n",
        "        if waveform.dim()>1:\n",
        "            waveform = waveform.mean(dim=0)\n",
        "        # return numpy 1D float32 (feature extractor accepts list of arrays)\n",
        "        wav_np = waveform.numpy().astype(np.float32)\n",
        "        # phonemes\n",
        "        phs = sentence_to_phonemes(transcript)\n",
        "        feat_seqs = phonemes_to_feature_sequences(phs)\n",
        "        return wav_np, phs, feat_seqs, path\n",
        "\n",
        "# collate: use feature-extractor `fe` to get input_values & attention_mask (pt tensors)\n",
        "def collate_batch(batch):\n",
        "    wavs = [item[0] for item in batch]  # numpy arrays\n",
        "    phs = [item[1] for item in batch]\n",
        "    feat_seqs = [item[2] for item in batch]\n",
        "    paths = [item[3] for item in batch]\n",
        "    inputs = fe(wavs, sampling_rate=fe.sampling_rate, return_tensors=\"pt\", padding=True)\n",
        "    return inputs, feat_seqs, phs, paths\n",
        "\n",
        "# --- create small dataloader (2 examples) ---\n",
        "ds = SmallLibriDataset(MANIFEST, max_items=200)   # limit to 200 reading time\n",
        "print(\"Dataset size (using max_items=200):\", len(ds))\n",
        "dl = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# --- build linear head and device setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "# freeze feature extractor of wav2vec if not already\n",
        "for p in model.feature_extractor.parameters():\n",
        "    p.requires_grad = False\n",
        "hidden = model.config.hidden_size\n",
        "out_dim = 2 * N_FEATURES + 1\n",
        "linear_head = nn.Linear(hidden, out_dim).to(device)\n",
        "model.to(device).eval()\n",
        "\n",
        "# take one batch and run forward\n",
        "batch_inputs, batch_feat_seqs, batch_phs, batch_paths = next(iter(dl))\n",
        "input_values = batch_inputs[\"input_values\"].to(device)   # shape (B, L)\n",
        "attention_mask = batch_inputs.get(\"attention_mask\", None)\n",
        "if attention_mask is not None:\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    wav2_out = model(input_values, attention_mask=attention_mask)\n",
        "    hs = wav2_out.last_hidden_state     # (B, T, H)\n",
        "    logits = linear_head(hs)            # (B, T, C)\n",
        "\n",
        "print(\"Batch audio paths:\", batch_paths)\n",
        "print(\"Example phonemes (sample 0):\", batch_phs[0][:60])\n",
        "print(\"Example feature-seq lengths for sample 0 (per-feature):\", [len(s) for s in batch_feat_seqs[0][:8]], \" ...\")\n",
        "print(\"Hidden states shape (B, T, H):\", hs.shape)\n",
        "print(\"Logits shape (B, T, C):\", logits.shape, \" expected C:\", out_dim)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:19:44.312185Z",
          "iopub.execute_input": "2025-11-07T12:19:44.312423Z",
          "iopub.status.idle": "2025-11-07T12:19:46.773525Z",
          "shell.execute_reply.started": "2025-11-07T12:19:44.312406Z",
          "shell.execute_reply": "2025-11-07T12:19:46.772862Z"
        },
        "id": "9ZD87UL398Hq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Kaggle notebook cell: create features_mapping.py and load mapping ===\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"consonantal\", \"sonorant\", \"approximant\", \"nasal\", \"voice\",\n",
        "    \"labial\", \"coronal\", \"anterior\", \"distributed\", \"dorsal\",\n",
        "    \"high\", \"low\", \"front\", \"back\", \"round\",\n",
        "    \"continuant\", \"delayed_release\", \"lateral\", \"strident\",\n",
        "    \"tense\", \"long\", \"stress\", \"syllabic\",\n",
        "    \"spread_glottis\", \"constricted_glottis\",\n",
        "    \"burst\", \"aspirated\", \"glide\", \"tap\", \"trill\",\n",
        "    \"stop\", \"fricative\", \"affricate\", \"vowel\", \"silence\"\n",
        "]\n",
        "\n",
        "# PHONEME_TO_FEATURES: map ARPAbet phonemes (no stress digits) -> set of +att features\n",
        "PHONEME_TO_FEATURES = {\n",
        "    # Vowels (approximate feature sets mapped into the 35 features above)\n",
        "    \"AA\": {\"vowel\",\"low\",\"back\",\"tense\",\"syllabic\"},\n",
        "    \"AE\": {\"vowel\",\"low\",\"front\",\"tense\",\"syllabic\"},\n",
        "    \"AH\": {\"vowel\",\"low\",\"syllabic\"},            # central-ish low vowel\n",
        "    \"AO\": {\"vowel\",\"low\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"AW\": {\"vowel\",\"low\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"AY\": {\"vowel\",\"low\",\"front\",\"syllabic\"},\n",
        "    \"EH\": {\"vowel\",\"front\",\"syllabic\"},\n",
        "    \"ER\": {\"vowel\",\"syllabic\"},                  # rhotic handled as vowel-like here\n",
        "    \"EY\": {\"vowel\",\"front\",\"syllabic\"},\n",
        "    \"IH\": {\"vowel\",\"high\",\"front\",\"syllabic\"},\n",
        "    \"IY\": {\"vowel\",\"high\",\"front\",\"tense\",\"syllabic\"},\n",
        "    \"OW\": {\"vowel\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"OY\": {\"vowel\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"UH\": {\"vowel\",\"high\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"UW\": {\"vowel\",\"high\",\"back\",\"round\",\"tense\",\"syllabic\"},\n",
        "\n",
        "    # Stops\n",
        "    \"P\": {\"consonantal\",\"labial\",\"anterior\",\"stop\"},\n",
        "    \"B\": {\"consonantal\",\"labial\",\"anterior\",\"stop\",\"voice\"},\n",
        "    \"T\": {\"consonantal\",\"coronal\",\"anterior\",\"stop\"},\n",
        "    \"D\": {\"consonantal\",\"coronal\",\"anterior\",\"stop\",\"voice\"},\n",
        "    \"K\": {\"consonantal\",\"dorsal\",\"stop\"},\n",
        "    \"G\": {\"consonantal\",\"dorsal\",\"stop\",\"voice\"},\n",
        "\n",
        "    # Affricates\n",
        "    \"CH\": {\"consonantal\",\"coronal\",\"anterior\",\"affricate\",\"delayed_release\"},\n",
        "    \"JH\": {\"consonantal\",\"coronal\",\"anterior\",\"affricate\",\"delayed_release\",\"voice\"},\n",
        "\n",
        "    # Fricatives\n",
        "    \"F\": {\"consonantal\",\"labial\",\"anterior\",\"fricative\",\"continuant\"},\n",
        "    \"V\": {\"consonantal\",\"labial\",\"anterior\",\"fricative\",\"continuant\",\"voice\"},\n",
        "    \"TH\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"distributed\"},\n",
        "    \"DH\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"distributed\",\"voice\"},\n",
        "    \"S\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"strident\",\"continuant\"},\n",
        "    \"Z\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"strident\",\"continuant\",\"voice\"},\n",
        "    \"SH\": {\"consonantal\",\"coronal\",\"distributed\",\"fricative\",\"strident\",\"continuant\"},\n",
        "    \"ZH\": {\"consonantal\",\"coronal\",\"distributed\",\"fricative\",\"strident\",\"continuant\",\"voice\"},\n",
        "    \"HH\": {\"consonantal\",\"fricative\",\"spread_glottis\"},\n",
        "\n",
        "    # Nasals\n",
        "    \"M\": {\"consonantal\",\"labial\",\"anterior\",\"nasal\",\"voice\"},\n",
        "    \"N\": {\"consonantal\",\"coronal\",\"anterior\",\"nasal\",\"voice\"},\n",
        "    \"NG\": {\"consonantal\",\"dorsal\",\"nasal\",\"voice\"},\n",
        "\n",
        "    # Liquids / approximants\n",
        "    \"L\": {\"consonantal\",\"coronal\",\"anterior\",\"lateral\",\"approximant\",\"voice\"},\n",
        "    \"R\": {\"consonantal\",\"coronal\",\"approximant\",\"voice\"},\n",
        "\n",
        "    # Glides\n",
        "    \"Y\": {\"consonantal\",\"dorsal\",\"high\",\"front\",\"glide\",\"approximant\",\"voice\"},\n",
        "    \"W\": {\"consonantal\",\"labial\",\"dorsal\",\"high\",\"back\",\"round\",\"glide\",\"approximant\",\"voice\"},\n",
        "\n",
        "    # Taps/trills/other\n",
        "    \"DX\": {\"consonantal\",\"coronal\",\"tap\",\"voice\"},\n",
        "    \"Q\": {\"consonantal\",\"stop\"},   # glottal stop marker if present\n",
        "    \"SIL\": {\"silence\"}\n",
        "}\n",
        "\n",
        "# Ensure every phoneme's features are a subset of FEATURE_ORDER (sanity)\n",
        "for ph, feats in list(PHONEME_TO_FEATURES.items()):\n",
        "    unseen = [f for f in feats if f not in FEATURE_ORDER]\n",
        "    if unseen:\n",
        "        raise ValueError(f\"Feature(s) {unseen} for phoneme {ph} are not in FEATURE_ORDER\")\n",
        "\n",
        "# Write to features_mapping.py file so other scripts can import it\n",
        "out = Path(\"features_mapping.py\")\n",
        "out.write_text(\n",
        "    \"# Auto-generated by Kaggle notebook cell\\n\"\n",
        "    \"FEATURE_ORDER = \" + repr(FEATURE_ORDER) + \"\\n\\n\"\n",
        "    \"PHONEME_TO_FEATURES = \" + repr(PHONEME_TO_FEATURES) + \"\\n\"\n",
        ")\n",
        "\n",
        "# Also expose mapping in the current notebook namespace\n",
        "print(\"Wrote features_mapping.py to\", out.resolve())\n",
        "print(\"FEATURE_ORDER length:\", len(FEATURE_ORDER))\n",
        "print(\"Example mapping for 'P':\", PHONEME_TO_FEATURES.get(\"P\"))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:19:46.774275Z",
          "iopub.execute_input": "2025-11-07T12:19:46.774503Z",
          "iopub.status.idle": "2025-11-07T12:19:46.791859Z",
          "shell.execute_reply.started": "2025-11-07T12:19:46.774484Z",
          "shell.execute_reply": "2025-11-07T12:19:46.791174Z"
        },
        "id": "Zo0LjMwq98Hr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic + Fix cell: fix transcripts and load local model\n",
        "import os, glob, json\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Edit these if your dataset names differ ----------\n",
        "# Where your downloaded local wav2vec2 files live (from your screenshot)\n",
        "LOCAL_MODEL_DIR = \"/kaggle/input/wav2vec2-large-robust-local/wav2vec2-large-robust-local\"\n",
        "# Where the Kaggle LibriSpeech input folder is\n",
        "LIBRISPEECH_ROOT = \"/kaggle/input/librispeech-100/LibriSpeech/train-clean-100\"\n",
        "# Where your manifest is (the TSV you generated earlier)\n",
        "MANIFEST = \"/kaggle/working/manifests/train-clean-100.tsv\"\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "print(\"Exists LOCAL_MODEL_DIR:\", os.path.exists(LOCAL_MODEL_DIR))\n",
        "print(\"Exists LIBRISPEECH_ROOT:\", os.path.exists(LIBRISPEECH_ROOT))\n",
        "print(\"Exists MANIFEST:\", os.path.exists(MANIFEST))\n",
        "print()\n",
        "\n",
        "# 1) show a few files in the model dir\n",
        "if os.path.exists(LOCAL_MODEL_DIR):\n",
        "    print(\"Local model directory listing (first 20):\")\n",
        "    for i, f in enumerate(sorted(os.listdir(LOCAL_MODEL_DIR))):\n",
        "        print(\" \", f)\n",
        "        if i >= 19:\n",
        "            break\n",
        "    # check for expected files\n",
        "    for expected in (\"pytorch_model.bin\", \"config.json\", \"preprocessor_config.json\"):\n",
        "        print(\"  has\", expected, \"=>\", os.path.exists(os.path.join(LOCAL_MODEL_DIR, expected)))\n",
        "print()\n",
        "\n",
        "# 2) find transcript files in LibriSpeech folder\n",
        "trans_files = glob.glob(os.path.join(LIBRISPEECH_ROOT, \"**/*.trans.txt\"), recursive=True)\n",
        "print(\"Found trans files (count):\", len(trans_files))\n",
        "if len(trans_files) > 0:\n",
        "    print(\"Example trans file:\", trans_files[0])\n",
        "    with open(trans_files[0], \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "        for i, line in enumerate(fh):\n",
        "            if i >= 5: break\n",
        "            print(\"  \", line.strip())\n",
        "else:\n",
        "    # also try uppercase TXT (some Kaggle packs use uppercase)\n",
        "    trans_files2 = glob.glob(os.path.join(LIBRISPEECH_ROOT, \"**/*.TXT\"), recursive=True)\n",
        "    print(\"Found uppercase .TXT files (count):\", len(trans_files2))\n",
        "    if len(trans_files2) > 0:\n",
        "        print(\"Example:\", trans_files2[0])\n",
        "        with open(trans_files2[0], \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "            for i, line in enumerate(fh):\n",
        "                if i >= 5: break\n",
        "                print(\"  \", line.strip())\n",
        "\n",
        "# 3) If manifest root is a placeholder, replace it with LIBRISPEECH_ROOT\n",
        "if os.path.exists(MANIFEST):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(MANIFEST, sep=\"\\t\", header=None, dtype=str)\n",
        "    root_print = df.iloc[0,0]\n",
        "    print(\"\\nManifest first-line root:\", root_print)\n",
        "    if root_print.strip().lower() in (\"audio_path\", \"path\", \"\"):\n",
        "        print(\"Manifest root looks like a placeholder. Rewriting manifest paths to use actual LIBRISPEECH_ROOT.\")\n",
        "        # Build new dataframe using actual root\n",
        "        df = df.iloc[1:].reset_index(drop=True)\n",
        "        df.columns = [\"relpath\", \"nframes\"]\n",
        "        df[\"path\"] = df[\"relpath\"].apply(lambda p: os.path.join(LIBRISPEECH_ROOT, p))\n",
        "        # save a fixed manifest for training\n",
        "        fixed_manifest = \"/kaggle/working/manifests/train-clean-100.fixed.tsv\"\n",
        "        # write with first line = actual root, then relative paths (we'll store absolute paths)\n",
        "        with open(fixed_manifest, \"w\") as out:\n",
        "            out.write(LIBRISPEECH_ROOT + \"\\n\")\n",
        "            for _, r in df.iterrows():\n",
        "                out.write(f\"{r['relpath']}\\t{r['nframes']}\\n\")\n",
        "        print(\"Wrote fixed manifest to:\", fixed_manifest)\n",
        "    else:\n",
        "        print(\"Manifest root appears valid:\", root_print)\n",
        "else:\n",
        "    print(\"Manifest file not found at\", MANIFEST)\n",
        "\n",
        "# 4) Build transcripts dict from trans files (robust to .trans.txt or .TXT)\n",
        "transcripts = {}\n",
        "for tf in trans_files + glob.glob(os.path.join(LIBRISPEECH_ROOT, \"**/*.TXT\"), recursive=True):\n",
        "    try:\n",
        "        with open(tf, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "            for line in fh:\n",
        "                parts = line.strip().split(\" \", 1)\n",
        "                if len(parts) == 2:\n",
        "                    utt, txt = parts\n",
        "                    transcripts[utt] = txt.lower()\n",
        "    except Exception as e:\n",
        "        print(\"skip\", tf, \"->\", e)\n",
        "\n",
        "print(\"\\nTotal transcripts loaded:\", len(transcripts))\n",
        "if len(transcripts) == 0:\n",
        "    print(\">> No transcripts found. Double-check LIBRISPEECH_ROOT path or dataset contents.\")\n",
        "else:\n",
        "    # show a couple of mappings\n",
        "    i = 0\n",
        "    for k in list(transcripts.keys())[:5]:\n",
        "        print(\" \", k, \"->\", transcripts[k][:80])\n",
        "        i += 1\n",
        "\n",
        "# 5) Load local model & feature extractor from LOCAL_MODEL_DIR (if exists)\n",
        "print()\n",
        "if os.path.exists(LOCAL_MODEL_DIR):\n",
        "    from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "    try:\n",
        "        print(\"Loading model from local dir (this may print many messages)...\")\n",
        "        backbone = Wav2Vec2Model.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
        "        feat = Wav2Vec2FeatureExtractor.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
        "        print(\"Loaded local model and feature extractor OK.\")\n",
        "        print(\"Model hidden_size:\", backbone.config.hidden_size)\n",
        "    except Exception as e:\n",
        "        print(\"Error loading local model:\", e)\n",
        "        print(\"Check that LOCAL_MODEL_DIR path points to directory containing 'pytorch_model.bin' and config.json\")\n",
        "else:\n",
        "    print(\"LOCAL_MODEL_DIR not found - cannot load local model. Make sure path is correct.\")\n",
        "\n",
        "print(\"\\nDone. If transcripts=0 or model didn't load, adjust LIBRISPEECH_ROOT and LOCAL_MODEL_DIR variables above and re-run this cell.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:19:46.792756Z",
          "iopub.execute_input": "2025-11-07T12:19:46.793049Z",
          "iopub.status.idle": "2025-11-07T12:20:01.295515Z",
          "shell.execute_reply.started": "2025-11-07T12:19:46.793023Z",
          "shell.execute_reply": "2025-11-07T12:20:01.294874Z"
        },
        "id": "LUo4zYFM98Hs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust manifest parser (replace the failing pd.read_csv)\n",
        "FIXED_MANIFEST='/kaggle/working/manifests/train-clean-100.fixed.tsv'\n",
        "FALLBACK_MANIFEST='/kaggle/working/manifests/train-clean-100.tsv'\n",
        "manifest_path = FIXED_MANIFEST if os.path.exists(FIXED_MANIFEST) else FALLBACK_MANIFEST\n",
        "print(\"Parsing manifest:\", manifest_path)\n",
        "\n",
        "root = None\n",
        "rows = []\n",
        "with open(manifest_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "    for i, raw in enumerate(fh):\n",
        "        line = raw.strip()\n",
        "        if line == \"\":\n",
        "            continue\n",
        "        if i == 0:\n",
        "            # first non-empty line is the root path\n",
        "            root = line\n",
        "            continue\n",
        "        # robust split: relpath and the rest (nframes or anything else)\n",
        "        parts = line.split(None, 1)  # split on any whitespace, max 1 split\n",
        "        if len(parts) == 0:\n",
        "            continue\n",
        "        elif len(parts) == 1:\n",
        "            rel = parts[0]\n",
        "            nframes = \"\"\n",
        "        else:\n",
        "            rel, nframes = parts[0], parts[1]\n",
        "        rows.append((rel, nframes))\n",
        "\n",
        "# Build dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(rows, columns=[\"relpath\", \"nframes\"])\n",
        "if root is None:\n",
        "    raise RuntimeError(\"Manifest root could not be read. Check the manifest file.\")\n",
        "df[\"path\"] = df[\"relpath\"].apply(lambda p: os.path.join(root, p))\n",
        "print(\"Parsed entries:\", len(df))\n",
        "print(\"Example rows:\")\n",
        "print(df.head(6))\n",
        "\n",
        "# quick sanity checks\n",
        "missing_paths = (~df[\"path\"].apply(os.path.exists)).sum()\n",
        "print(f\"Files not found on disk (approx): {missing_paths} (expected if manifest used relative paths that we rewrote)\")\n",
        "\n",
        "# attach transcripts (existing code expects df after this), so leave df in scope"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:21:51.901753Z",
          "iopub.execute_input": "2025-11-07T12:21:51.902027Z",
          "iopub.status.idle": "2025-11-07T12:22:09.939938Z",
          "shell.execute_reply.started": "2025-11-07T12:21:51.902008Z",
          "shell.execute_reply": "2025-11-07T12:22:09.939151Z"
        },
        "id": "xcoLF0Mr98Hu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: create features_mapping.py and expose FEATURE_ORDER and PHONEME_TO_FEATURES\n",
        "from pathlib import Path\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"consonantal\", \"sonorant\", \"approximant\", \"nasal\", \"voice\",\n",
        "    \"labial\", \"coronal\", \"anterior\", \"distributed\", \"dorsal\",\n",
        "    \"high\", \"low\", \"front\", \"back\", \"round\",\n",
        "    \"continuant\", \"delayed_release\", \"lateral\", \"strident\",\n",
        "    \"tense\", \"long\", \"stress\", \"syllabic\",\n",
        "    \"spread_glottis\", \"constricted_glottis\",\n",
        "    \"burst\", \"aspirated\", \"glide\", \"tap\", \"trill\",\n",
        "    \"stop\", \"fricative\", \"affricate\", \"vowel\", \"silence\"\n",
        "]\n",
        "\n",
        "# Minimal comprehensive mapping for CMU ARPAbet symbols commonly in LibriSpeech\n",
        "PHONEME_TO_FEATURES = {\n",
        "    # Vowels\n",
        "    \"AA\": {\"vowel\",\"low\",\"back\",\"tense\",\"syllabic\"},\n",
        "    \"AE\": {\"vowel\",\"low\",\"front\",\"tense\",\"syllabic\"},\n",
        "    \"AH\": {\"vowel\",\"low\",\"syllabic\"},\n",
        "    \"AO\": {\"vowel\",\"low\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"AW\": {\"vowel\",\"low\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"AY\": {\"vowel\",\"low\",\"front\",\"syllabic\"},\n",
        "    \"EH\": {\"vowel\",\"front\",\"syllabic\"},\n",
        "    \"ER\": {\"vowel\",\"syllabic\"},\n",
        "    \"EY\": {\"vowel\",\"front\",\"syllabic\"},\n",
        "    \"IH\": {\"vowel\",\"high\",\"front\",\"syllabic\"},\n",
        "    \"IY\": {\"vowel\",\"high\",\"front\",\"tense\",\"syllabic\"},\n",
        "    \"OW\": {\"vowel\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"OY\": {\"vowel\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"UH\": {\"vowel\",\"high\",\"back\",\"round\",\"syllabic\"},\n",
        "    \"UW\": {\"vowel\",\"high\",\"back\",\"round\",\"tense\",\"syllabic\"},\n",
        "\n",
        "    # Stops\n",
        "    \"P\": {\"consonantal\",\"labial\",\"anterior\",\"stop\"},\n",
        "    \"B\": {\"consonantal\",\"labial\",\"anterior\",\"stop\",\"voice\"},\n",
        "    \"T\": {\"consonantal\",\"coronal\",\"anterior\",\"stop\"},\n",
        "    \"D\": {\"consonantal\",\"coronal\",\"anterior\",\"stop\",\"voice\"},\n",
        "    \"K\": {\"consonantal\",\"dorsal\",\"stop\"},\n",
        "    \"G\": {\"consonantal\",\"dorsal\",\"stop\",\"voice\"},\n",
        "\n",
        "    # Affricates\n",
        "    \"CH\": {\"consonantal\",\"coronal\",\"anterior\",\"affricate\",\"delayed_release\"},\n",
        "    \"JH\": {\"consonantal\",\"coronal\",\"anterior\",\"affricate\",\"delayed_release\",\"voice\"},\n",
        "\n",
        "    # Fricatives\n",
        "    \"F\": {\"consonantal\",\"labial\",\"anterior\",\"fricative\",\"continuant\"},\n",
        "    \"V\": {\"consonantal\",\"labial\",\"anterior\",\"fricative\",\"continuant\",\"voice\"},\n",
        "    \"TH\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"distributed\"},\n",
        "    \"DH\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"distributed\",\"voice\"},\n",
        "    \"S\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"strident\",\"continuant\"},\n",
        "    \"Z\": {\"consonantal\",\"coronal\",\"anterior\",\"fricative\",\"strident\",\"continuant\",\"voice\"},\n",
        "    \"SH\": {\"consonantal\",\"coronal\",\"distributed\",\"fricative\",\"strident\",\"continuant\"},\n",
        "    \"ZH\": {\"consonantal\",\"coronal\",\"distributed\",\"fricative\",\"strident\",\"continuant\",\"voice\"},\n",
        "    \"HH\": {\"consonantal\",\"fricative\",\"spread_glottis\"},\n",
        "\n",
        "\n",
        "    # Nasals\n",
        "    \"M\": {\"consonantal\",\"labial\",\"anterior\",\"nasal\",\"voice\"},\n",
        "    \"N\": {\"consonantal\",\"coronal\",\"anterior\",\"nasal\",\"voice\"},\n",
        "    \"NG\": {\"consonantal\",\"dorsal\",\"nasal\",\"voice\"},\n",
        "\n",
        "    # Liquids / approximants\n",
        "    \"L\": {\"consonantal\",\"coronal\",\"anterior\",\"lateral\",\"approximant\",\"voice\"},\n",
        "    \"R\": {\"consonantal\",\"coronal\",\"approximant\",\"voice\"},\n",
        "\n",
        "    # Glides\n",
        "    \"Y\": {\"consonantal\",\"dorsal\",\"high\",\"front\",\"glide\",\"approximant\",\"voice\"},\n",
        "    \"W\": {\"consonantal\",\"labial\",\"dorsal\",\"high\",\"back\",\"round\",\"glide\",\"approximant\",\"voice\"},\n",
        "\n",
        "    # Others / special / tokens\n",
        "    \"DX\": {\"consonantal\",\"coronal\",\"tap\",\"voice\"},\n",
        "    \"Q\": {\"consonantal\",\"stop\"},   # glottal stop marker if present\n",
        "    \"SIL\": {\"silence\"}\n",
        "}\n",
        "\n",
        "# Sanity: check all features listed are in FEATURE_ORDER\n",
        "_unseen = []\n",
        "for ph, feats in PHONEME_TO_FEATURES.items():\n",
        "    for f in feats:\n",
        "        if f not in FEATURE_ORDER:\n",
        "            _unseen.append((ph,f))\n",
        "if _unseen:\n",
        "    raise RuntimeError(f\"Found features not in FEATURE_ORDER: {_unseen[:10]}\")\n",
        "\n",
        "# Write file for import by other cells\n",
        "out = Path(\"features_mapping.py\")\n",
        "out.write_text(\n",
        "    \"# Auto-generated by notebook cell\\n\"\n",
        "    \"FEATURE_ORDER = \" + repr(FEATURE_ORDER) + \"\\n\\n\"\n",
        "    \"PHONEME_TO_FEATURES = \" + repr(PHONEME_TO_FEATURES) + \"\\n\"\n",
        ")\n",
        "print(\"Wrote features_mapping.py (FEATURE_ORDER len = {})\".format(len(FEATURE_ORDER)))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:26:38.898884Z",
          "iopub.execute_input": "2025-11-07T12:26:38.89921Z",
          "iopub.status.idle": "2025-11-07T12:26:38.91216Z",
          "shell.execute_reply.started": "2025-11-07T12:26:38.899184Z",
          "shell.execute_reply": "2025-11-07T12:26:38.911538Z"
        },
        "id": "whOyIesa98Hv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Single self-contained training cell =================\n",
        "# Copy-paste and run. Adjust SMALL config at top if needed.\n",
        "\n",
        "RUN_SMOKE_TEST = False   # True = run 1 training step and exit (useful to verify). Set False to run full training.\n",
        "LOCAL_MODEL_DIR = \"/kaggle/input/wav2vec2-large-robust-local/wav2vec2-large-robust-local\"\n",
        "FIXED_MANIFEST = \"/kaggle/working/manifests/train-clean-100.fixed.tsv\"\n",
        "LIBRISPEECH_ROOT = \"/kaggle/input/librispeech-100/LibriSpeech/train-clean-100\"\n",
        "SAVE_DIR = \"/kaggle/working/checkpoints\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# hyperparams\n",
        "SAMPLE_RATE = 16000\n",
        "BATCH_SIZE = 2       # safe default for smoke + GPU\n",
        "EPOCHS = 3\n",
        "ACCUM_STEPS = 1\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 0.005\n",
        "NUM_WORKERS = 0      # set 0 in Kaggle to avoid worker spawn issues\n",
        "\n",
        "# ---------- imports ----------\n",
        "try:\n",
        "    import pronouncing\n",
        "except Exception:\n",
        "    !pip install pronouncing\n",
        "    import pronouncing\n",
        "\n",
        "import os, glob, time, gc, random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Config, Wav2Vec2Model, Wav2Vec2FeatureExtractor, get_linear_schedule_with_warmup\n",
        "\n",
        "# ---- G2P (grapheme-to-phoneme) using cmudict ----\n",
        "import cmudict\n",
        "\n",
        "# Load CMU dictionary (about 134k words)\n",
        "cmu = cmudict.dict()\n",
        "print(\"Using CMUdict for G2P. Entries:\", len(cmu))\n",
        "\n",
        "def word_to_phones(word):\n",
        "    \"\"\"Return a list of phonemes for a given word, without stress markers.\"\"\"\n",
        "    w = word.lower()\n",
        "    if w in cmu:\n",
        "        phs = cmu[w][0]  # first pronunciation\n",
        "        return [p.rstrip(\"0123456789\") for p in phs]\n",
        "    return []  # unknown word\n",
        "\n",
        "def text_to_phonemes(text):\n",
        "    \"\"\"Convert a sentence into a flat phoneme sequence.\"\"\"\n",
        "    phones = []\n",
        "    for w in text.strip().split():\n",
        "        phones.extend(word_to_phones(w))\n",
        "    return phones\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ---------- feature mapping (cell1 must have created features_mapping.py) ----------\n",
        "from features_mapping import PHONEME_TO_FEATURES, FEATURE_ORDER\n",
        "N_FEATURES = len(FEATURE_ORDER)\n",
        "assert N_FEATURES == 35, f\"Expected 35 features, got {N_FEATURES}\"\n",
        "OUTPUT_DIM = N_FEATURES * 2 + 1\n",
        "CTC_BLANK_IDX = 2\n",
        "\n",
        "# ---------- robust manifest parsing ----------\n",
        "manifest_path = FIXED_MANIFEST\n",
        "print(\"Parsing manifest:\", manifest_path)\n",
        "root = None\n",
        "rows = []\n",
        "with open(manifest_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "    for raw in fh:\n",
        "        line = raw.strip()\n",
        "        if line == \"\":\n",
        "            continue\n",
        "        if root is None:\n",
        "            root = line\n",
        "            continue\n",
        "        parts = line.split(None, 1)\n",
        "        if len(parts) == 0:\n",
        "            continue\n",
        "        elif len(parts) == 1:\n",
        "            rel, nframes = parts[0], \"\"\n",
        "        else:\n",
        "            rel, nframes = parts[0], parts[1]\n",
        "        rows.append((rel, nframes))\n",
        "if root is None:\n",
        "    raise RuntimeError(\"Manifest root not found.\")\n",
        "df = pd.DataFrame(rows, columns=[\"relpath\", \"nframes\"])\n",
        "df[\"path\"] = df[\"relpath\"].apply(lambda p: os.path.join(root, p))\n",
        "print(\"Parsed entries:\", len(df))\n",
        "missing_files = (~df[\"path\"].apply(os.path.exists)).sum()\n",
        "print(\"Missing audio files (should be 0):\", missing_files)\n",
        "\n",
        "# ---------- transcripts ----------\n",
        "transcripts = {}\n",
        "for tf in glob.glob(os.path.join(LIBRISPEECH_ROOT, \"**/*.trans.txt\"), recursive=True) + \\\n",
        "          glob.glob(os.path.join(LIBRISPEECH_ROOT, \"**/*.TXT\"), recursive=True):\n",
        "    try:\n",
        "        with open(tf, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "            for line in fh:\n",
        "                parts = line.strip().split(\" \", 1)\n",
        "                if len(parts) == 2:\n",
        "                    utt, txt = parts\n",
        "                    transcripts[utt] = txt.lower()\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"Transcripts loaded:\", len(transcripts))\n",
        "\n",
        "def lookup_text(path):\n",
        "    utt = os.path.basename(path).split(\".\")[0]\n",
        "    return transcripts.get(utt, \"\")\n",
        "\n",
        "df[\"text\"] = df[\"path\"].apply(lookup_text)\n",
        "empty_count = (df[\"text\"] == \"\").sum()\n",
        "print(\"Utterances missing transcripts:\", empty_count)\n",
        "\n",
        "# ---------- G2P via pronouncing ----------\n",
        "def word_to_phones(word):\n",
        "    cand = pronouncing.phones_for_word(word.lower())\n",
        "    if not cand:\n",
        "        return []\n",
        "    phs = cand[0].split()\n",
        "    return [p.rstrip(\"0123456789\") for p in phs]\n",
        "\n",
        "def text_to_phonemes(text):\n",
        "    phones=[]\n",
        "    for w in text.strip().split():\n",
        "        phones.extend(word_to_phones(w))\n",
        "    return phones\n",
        "\n",
        "def phonemes_to_feature_sequences(phonemes):\n",
        "    seqs = [[] for _ in range(N_FEATURES)]\n",
        "    for ph in phonemes:\n",
        "        feats = PHONEME_TO_FEATURES.get(ph, set())\n",
        "        for i, fname in enumerate(FEATURE_ORDER):\n",
        "            seqs[i].append(0 if fname in feats else 1)\n",
        "    out=[]\n",
        "    for s in seqs:\n",
        "        if len(s)==0:\n",
        "            out.append(torch.LongTensor([1]))\n",
        "        else:\n",
        "            out.append(torch.LongTensor(s))\n",
        "    return out\n",
        "\n",
        "# ---------- dataset and loaders ----------\n",
        "class LibriPhonoDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        wav, sr = torchaudio.load(r.path)\n",
        "        if sr != SAMPLE_RATE:\n",
        "            wav = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(wav)\n",
        "        audio = wav.squeeze(0)\n",
        "        phones = text_to_phonemes(r.text)\n",
        "        feats = phonemes_to_feature_sequences(phones)\n",
        "        return {\"audio\": audio, \"features\": feats}\n",
        "\n",
        "# small split for speed\n",
        "split_idx = int(len(df) * 0.98)\n",
        "train_df = df.iloc[:split_idx].reset_index(drop=True)\n",
        "val_df = df.iloc[split_idx:].reset_index(drop=True)\n",
        "print(\"Train/Val sizes:\", len(train_df), len(val_df))\n",
        "\n",
        "train_ds = LibriPhonoDataset(train_df)\n",
        "val_ds = LibriPhonoDataset(val_df)\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    audios = [b[\"audio\"].numpy() for b in batch]\n",
        "    features = [b[\"features\"] for b in batch]\n",
        "    inputs = feature_extractor(audios, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\", padding=True)\n",
        "    return {\"input_values\": inputs.input_values, \"features\": features}\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n",
        "\n",
        "# ---------- safe model load (explicit CPU load, then move to GPU) ----------\n",
        "print(\"Safe-load: reading config from\", LOCAL_MODEL_DIR)\n",
        "cfg = Wav2Vec2Config.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
        "print(\"Config loaded. hidden_size =\", cfg.hidden_size)\n",
        "\n",
        "print(\"Instantiating model skeleton on CPU\")\n",
        "backbone = Wav2Vec2Model(cfg)\n",
        "backbone.cpu()\n",
        "\n",
        "state_path = os.path.join(LOCAL_MODEL_DIR, \"pytorch_model.bin\")\n",
        "if not os.path.exists(state_path):\n",
        "    raise FileNotFoundError(state_path)\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"Loading state dict from disk (map_location='cpu') ...\")\n",
        "state_dict = torch.load(state_path, map_location=\"cpu\")\n",
        "print(\"Loaded state dict keys:\", len(state_dict), \" time:\", time.time()-t0)\n",
        "\n",
        "print(\"Loading state dict into model (strict=False) ...\")\n",
        "res = backbone.load_state_dict(state_dict, strict=False)\n",
        "#print(\"load_state_dict result:\", res)   # prints missing/unexpected keys lists\n",
        "\n",
        "del state_dict\n",
        "gc.collect()\n",
        "\n",
        "# freeze CNN feature extractor\n",
        "for p in backbone.feature_extractor.parameters():\n",
        "    p.requires_grad = False\n",
        "print(\"Feature extractor frozen.\")\n",
        "\n",
        "print(\"Moving model to device:\", DEVICE)\n",
        "backbone.to(DEVICE)\n",
        "\n",
        "# attach linear head\n",
        "class PhonoModel(nn.Module):\n",
        "    def __init__(self, base, output_dim):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.linear = nn.Linear(base.config.hidden_size, output_dim)\n",
        "    def forward(self, x):\n",
        "        last = self.base(x).last_hidden_state\n",
        "        logits = self.linear(last)\n",
        "        return logits, last\n",
        "\n",
        "model = PhonoModel(backbone, OUTPUT_DIM).to(DEVICE)\n",
        "print(\"Model ready. OUTPUT_DIM:\", OUTPUT_DIM)\n",
        "\n",
        "# ---------- optimizer / scheduler / loss ----------\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "total_steps = max(1, (len(train_loader) * EPOCHS) // ACCUM_STEPS)\n",
        "warmup = max(1, int(total_steps * 0.10))\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup, num_training_steps=total_steps)\n",
        "ctc_loss = nn.CTCLoss(blank=CTC_BLANK_IDX, zero_infinity=True)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
        "\n",
        "# ---------- SCTC-SB loss ----------\n",
        "def sctc_sb_loss(logits, targets):\n",
        "    B,T,D = logits.shape\n",
        "    device = logits.device\n",
        "    blank_logits = logits[:,:, -1]\n",
        "    total = torch.tensor(0.0, device=device)\n",
        "    for i in range(N_FEATURES):\n",
        "        pos_idx = 2*i\n",
        "        neg_idx = 2*i + 1\n",
        "        cat_logits = torch.stack([logits[:,:,pos_idx], logits[:,:,neg_idx], blank_logits], dim=-1)\n",
        "        logp = F.log_softmax(cat_logits.permute(1,0,2), dim=2)\n",
        "        tgt_list=[]\n",
        "        tgt_lens=[]\n",
        "        for b in range(B):\n",
        "            t = targets[b][i]\n",
        "            tgt_list.append(t)\n",
        "            tgt_lens.append(len(t))\n",
        "        tgt_concat = torch.cat(tgt_list).to(device)\n",
        "        tgt_lens = torch.LongTensor(tgt_lens).to(device)\n",
        "        in_lens = torch.LongTensor([T]*B).to(device)\n",
        "        loss_i = ctc_loss(logp, tgt_concat, in_lens, tgt_lens)\n",
        "        total = total + loss_i\n",
        "    return total\n",
        "\n",
        "# ---------- training (or smoke test) ----------\n",
        "if RUN_SMOKE_TEST:\n",
        "    print(\"Running single-step smoke test...\")\n",
        "    model.train()\n",
        "    batch = next(iter(train_loader))\n",
        "    x = batch[\"input_values\"].to(DEVICE)\n",
        "    targets = batch[\"features\"]\n",
        "    with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "        logits, _ = model(x)\n",
        "        loss = sctc_sb_loss(logits, targets)\n",
        "    print(\"Smoke test loss:\", loss.item())\n",
        "    print(\"Smoke test completed  set RUN_SMOKE_TEST=False to run full training.\")\n",
        "else:\n",
        "    print(\"Starting full training...\")\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    best_val = float(\"inf\")\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch}/{EPOCHS}\")\n",
        "        running_loss = 0.0\n",
        "        for step, batch in enumerate(pbar):\n",
        "            inputs = batch[\"input_values\"].to(DEVICE)\n",
        "            targets = batch[\"features\"]\n",
        "            with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "                logits, _ = model(inputs)\n",
        "                loss = sctc_sb_loss(logits, targets)\n",
        "                loss_value = loss.item()\n",
        "            scaler.scale(loss / ACCUM_STEPS).backward()\n",
        "            if (step + 1) % ACCUM_STEPS == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "            running_loss += loss_value\n",
        "            pbar.set_postfix({\"loss\": f\"{running_loss / (step+1):.4f}\"})\n",
        "        # validation after epoch\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for vb, vbatch in enumerate(tqdm(val_loader, desc=\"Val\", leave=False)):\n",
        "                vinputs = vbatch[\"input_values\"].to(DEVICE)\n",
        "                vtargets = vbatch[\"features\"]\n",
        "                v_logits, _ = model(vinputs)\n",
        "                vl = sctc_sb_loss(v_logits, vtargets)\n",
        "                val_loss += vl.item()\n",
        "                val_steps += 1\n",
        "                if val_steps >= 100:\n",
        "                    break\n",
        "        val_loss_avg = val_loss / max(1, val_steps)\n",
        "        print(f\"Epoch {epoch} avg val loss: {val_loss_avg:.4f}\")\n",
        "        ckpt = os.path.join(SAVE_DIR, f\"phono_sctc_epoch{epoch}.pt\")\n",
        "        torch.save(model.state_dict(), ckpt)\n",
        "        print(\"Saved:\", ckpt)\n",
        "        if val_loss_avg < best_val:\n",
        "            best_val = val_loss_avg\n",
        "            best_ckpt = os.path.join(SAVE_DIR, \"phono_sctc_best.pt\")\n",
        "            torch.save(model.state_dict(), best_ckpt)\n",
        "            print(\"Saved new best:\", best_ckpt)\n",
        "        model.train()\n",
        "    print(\"Training finished. Best val loss:\", best_val)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-07T12:38:53.46403Z",
          "iopub.execute_input": "2025-11-07T12:38:53.464366Z",
          "iopub.status.idle": "2025-11-07T12:41:58.398429Z",
          "shell.execute_reply.started": "2025-11-07T12:38:53.464343Z",
          "shell.execute_reply": "2025-11-07T12:41:58.397448Z"
        },
        "id": "MF2RTc6u98Hx"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}